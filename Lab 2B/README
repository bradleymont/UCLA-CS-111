NAME: Bradley Mont
EMAIL: bradleymont@gmail.com
ID: 804993030

Description of Included Files

lab2_add.c: a C program that implements and tests a shared variable add function

SortedList.h: a header file describing the interfaces for linked list operations

SortedList.c: a C module that implements insert, delete, lookup, and length methods for a sorted doubly linked list

lab2_list.c: a C program that implements and tests different multithreaded linked list operations

Makefile: contains targets build, tests, graphs, dist, and clean to build the deliverable programs (lab2_add and lab2_list), output, graphs, and tarball

lab2_add.csv: contains all of the results for all of the Part-1 tests

lab2_list.csv: contains all of the results for all of the Part-2 tests

lab2_add-1.png: threads and iterations required to generate a failure (with and without yields)
lab2_add-2.png: average time per operation with and without yields
lab2_add-3.png: average time per (single threaded) operation vs. the number of iterations
lab2_add-4.png: threads and iterations that can run successfully with yields under each of the synchronization options
lab2_add-5.png: average time per (protected) operation vs. the number of threads

lab2_list-1.png: average time per (single threaded) unprotected operation vs. number of iterations (illustrating the correction of the per-operation cost for the list length)
lab2_list-2.png: threads and iterations required to generate a failure (with and without yields)
lab2_list-3.png: iterations that can run (protected) without failure
lab2_list-4.png: (length-adjusted) cost per operation vs the number of threads for the various synchronization options

README: this file (contains answers to lab questions)

lab2_test.sh: contains tests for lab2_add and lab2_list, appends output to .csv files

Questions

QUESTION 2.1.1 - causing conflicts:
Why does it take many iterations before errors are seen?
Why does a significantly smaller number of iterations so seldom fail?

It takes many iterations before errors are seen because for errors to occur, there must be at least 2 threads accessing the same resource. Therefore, the amount of iterations must be large enough so that other threads can start running while one thread is still finishing, thus creating the possibility for race conditions. A significantly smaller number of iterations so seldom fails because there is much less concurrency with fewer operations: since threads run for shorter, there is a smaller likelihood that multiple threads will access the same shared memory and cause race conditions.


QUESTION 2.1.2 - cost of yielding:
Why are the --yield runs so much slower?
Where is the additional time going?
Is it possible to get valid per-operation timings if we are using the --yield option?
If so, explain how. If not, explain why not.

The --yield runs are so much slower because yield causes a context switch, so all the context switches add a lot of overhead and slow down the program. The additional time is going toward saving and restoring the state of each thread involved in each context switch.
It is not possible to get valid per-operation timings if we are using the -yield option because our timing would include the time for the context switches as well. Even if there were a way to accurately measure the time for each operation, it would result in inaccurate times because add is such a cheap operation that any extra overhead (such as measuring and calculating time) could overshadow the actual time taken for the add operation.


QUESTION 2.1.3 - measurement errors:
Why does the average cost per operation drop with increasing iterations?
If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?

The average cost per operation drops with increasing iterations because when we measure time (cost), we are also measuring the time to create each thread, which causes extra overhead. Therefore, the average cost per operation goes down with increasing iterations because the cost for creating the threads eventually becomes drowned out compared to the large cost of more iterations.
If the cost per iteration is a function of the number of iterations, we would technically have to run an infinite (or extremely large until the thread creation cost is amortized) amount of iterations to calculate the "correct" cost because this would cause the overhead of creating threads to be insignificant in comparison to the actual operations.


QUESTION 2.1.4 - costs of serialization:
Why do all of the options perform similarly for low numbers of threads?
Why do the three protected operations slow down as the number of threads rises?

All of the options perform similarly for low numbers of threads because two main factors for performance are adding as well as blocking. A low number of threads means that the cost of blocking is relatively small because there are less threads to wait for when a thread wants access to a critical section; therefore, the performance for each thread will mostly be based on executing the add option, which will be similar for each thread.
As the number of threads rises, the three protected operations slow down because each thread will be blocked longer waiting for access to the critical section. With more threads, there's a greater chance of multiple threads contending for the same shared memory, so threads will have to wait longer to acquire the lock for a critical section.


QUESTION 2.2.1 - scalability of Mutex
Compare the variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists).
Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

In both parts, the time per mutex-protected operation increases as the number of threads increases. However, in Part-2, the time per mutex-protected operation increases at a much faster rate because the basic operation for Part-2 (manipulating a sorted linked list) is much more expensive than the basic operation for Part-1 (adding to a shared variable); therefore, the chance of multiple threads contending for the same resource increases because they spend more time in the critical section, so Part-2 increases at a faster rate. 
This explains the general shapes of the curves: both parts have a positive correlation between time per mutex-protected operation and the number of threads, but the graph for Part-2 increases at a higher rate. 


QUESTION 2.2.2 - scalability of spin locks

Compare the variation in time per protected operation vs the number of threads for list operations protected by Mutex vs Spin locks. Comment on the general shapes of the curves, and explain why they have this shape.
Comment on the relative rates of increase and differences in the shapes of the curves, and offer an explanation for these differences.

As the number of threads for list operations increases, the time per protected operation increases for both types of locks; however, the time per protected operation for spin locks increases at a much faster rate than that of mutex locks. This occurs because mutex locks put threads to sleep while they wait to acquire the lock for a critical section, but spin locks make the thread keep spinning until it acquires the lock to enter the critical section. This spinning adds extra overhead and CPU time that mutex locks do not have. This explains the general shapes of the curves: mutex and spin locks both increase their time per protected operation as the number of threads for list operations increases, but the graphs for spin locks increase at a higher rate due to the increased CPU time taken by spin locks spinning while they wait for the lock.
