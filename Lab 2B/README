NAME: Bradley Mont
EMAIL: bradleymont@gmail.com
ID: 804993030

Description of Included Files

SortedList.h: a header file describing the interfaces for linked list operations

SortedList.c: a C module that implements insert, delete, lookup, and length methods for a sorted doubly linked list

lab2_list.c: a C program that implements and tests different multithreaded linked list operations

Makefile: contains targets build, tests, profile, graphs, dist, and clean to build the deliverable programs, output, graphs, and tarball

lab2b_list.csv: contains all of the results for all of test runs

profile.out - execution profiling report showing where time was spent in the un-partitioned spin-lock implementation

lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized list operations.
lab2b_2.png: mean time per mutex wait and mean time per operation for mutex-synchronized list operations.
lab2b_3.png: successful iterations vs. threads for each synchronization method.
lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists.
lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.

README: this file (contains answers to lab questions)

lab2_test.sh: contains tests for lab2_list, appends output to .csv file

Questions

QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

For 1-thread and spin-lock synchronization, most of the CPU time goes toward list operations since the single thread does not have to do any spinning before it acquires the lock, so it can spend a majority of its time doing the actual list operations. This applies to both a small and large amount of iterations; the time spent spinning will be almost nothing since the lock will be immediately available, so the CPU time can be spent on list operations. 

For 1-thread and mutex synchronization, the behavior for a small amount of iterations cannot be predicted because we are unsure how the time spent locking and unlocking the mutex lock compares to the time spent doing list operations. Either one of them could run faster with a small amount of iterations. However, with a large amount of iterations, the time spent doing list operations will overshadow the time spent locking and unlocking the mutex lock, so most of the CPU time will be spent on list operations for 1-thread, mutex synchronization, and a large amount of iterations.

For 2-thread, spin-lock synchronization, and a small amount of iterations, the CPU time will be spent mostly on list operations and partly on spinning. While one thread is executing list operations, the other will be spinning. However, due to the extra overhead of setting up the locks, most of the CPU time will be spent on spinning in this case, with a decent amount going toward list operations. But with a large amount of iterations, the overhead of setting up the locks will get overshadowed, so the CPU time will be split pretty evenly between spinning and list operations because one thread spins while the other one executes list operations.

For 2-thread, mutex synchronization, and a small amount of iterations, we still are unsure because we don't know how the time spent locking and unlocking the mutex lock compares to the time spent doing list operations. However, with a large amount of iterations, most of the CPU time will be spent on list operations due to the same reason as the single threaded case.

In the high-thread spin-lock tests, I believe most of the CPU time is being spent spinning because a majority of the threads will spin (wait for the lock) while one thread is doing list operations.

In the high-thread mutex tests, I believe that if the lists are decently short and the list operations are very quick, then a majority of the CPU time would go toward context-switching between threads, which can be very expensive. However, if the lists are extremely long and list operations become very expensive, more CPU time could be spent on list operations opposed to context switches.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

After using gperftools to profile the spin-lock version of the list exerciser with a large number of threads, I see that my lock function is consuming most of the CPU time in this scenario. Within this lock function, I set up my locking synchronization for the list operations. Specifically, the line of code "while (__sync_lock_test_and_set(&sublist->mySpinLock, 1));" is taking up the most CPU time because this is the part of the cofe that keeps spinning and chewing up CPU time until the lock has been freed.

This operation becomes so expensive with a large number of threads because as the number of threads increases, there's more threads in contention for CPU time. Therefore, more threads will have to simply spin and use up CPU cycles as they wait for the lock to the critical section to be unlocked so they can execute their list operations.


QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The average lock-wait time rises so dramatically with the number of contending threads because of two reasons. First, with more threads, there is more contention between threads for access to the critical section, so the likelihood of a thread having to wait increasese with the number of threads since only one thread can do list operations in the critical section at a time, so an increasing amount of threads must wait as we increase the number of threads. Additionally, with more threads, the longer the queue gets for threads waiting for the mutex to be unlocked, so the longer waiting queue and increased contention causes the average lock-wait time to rise so dramatically with the number of contending threads.

The completion time per operation rises (less dramatically) with the number of contending threads because as the number of threads increases, more CPU time is spent on context switches between threads, so the completion time per operation will go up. Additionally, with more threads, there's more items in the list (threads x iterations), so each list operation will take longer as a result (counting more items, deleting more items, etc).

It is possible for the wait time per operation to go up faster (or higher) than the completion time per operation. First, note that the wait time measures the per-thread average time for an operation, while the completion time per operation is the total average amount of time per operation. Therefore, multiple threads can be waiting at the same time (their wait times can overlap); this means that the wait time per operation can go up faster than the completion time per operation since the wait time will be adding up overlapping wait times for threads, while the completion time will not. 


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

As the number of lists increases, the performance of the synchronized methods increases as well because as the number of lists goes up, the contention for shared resources between threads goes down. More specifically, with just one list, only one thread can be operating on it at a time, while all the other threads must wait. However, as we increase the number of sub-lists, one thread can be working on each sub-list at a time without causing race conditions, so more threads can be executing list operations currently; thus, performance goes up with the number of lists.

As the number of lists is further increased, the throughput should continue increasing until a certain point when the likelihood that two threads will be in contention for the same resource will be basically zero; at this point, there will be no benefit (throughput increase) when we increase the number of lists. Additionally, we could reach a point where the overhead of creating a large number of sublists is greater than the throughput benefits it provides.

No, this does not appear to be true in the above curves. As we partition a list into more and more sub-lists, each sub-list has a smaller amount of items in it that need to be traversed by the different list operations. Therefore, due the smaller size of each sub-list, threads will need to spend less time in the critical section, so contention between threads will go down as well. With less contention and a smaller list to do operations on, the relative throughput of the N-way partitioned list is greater than the throughput of a single list with fewee (1/N) threads.